<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&amp;family=Merriweather:wght@400;700&amp;display=swap" rel="stylesheet"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha512-+ctbLgWY60CygPL9aS+6CXY0RKXZGZoc7ztWGLu3vX6p2EXD1+u7pHzGoElv9v4zXqa4xzT5E8+1mF1Qk7S/9g==" crossorigin="anonymous" referrerpolicy="no-referrer"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-N9TWYGZH3J"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-N9TWYGZH3J");</script> <title>Workshop Schedule | blank</title> <link rel="stylesheet" href="/assets/css/conference.css"> <style>*{box-sizing:border-box}html,body{margin:0;padding:0;font-family:'Inter',sans-serif;background-color:#f4f4f9;color:#333;scroll-behavior:smooth}.container{width:90%;max-width:1140px;margin:0 auto;padding:0 1rem}header.header{background:linear-gradient(90deg,#0056b3,#036);color:#fff;padding:1.5rem 0;box-shadow:0 4px 8px rgba(0,0,0,0.2)}.site-title{font-family:'Merriweather',serif;text-align:center;font-size:2.2rem;margin:0 0 .5rem}nav.nav{display:flex;flex-wrap:wrap;justify-content:center;gap:1rem}nav.nav a.nav-link{color:#fff;text-decoration:none;padding:.6rem 1.2rem;border-radius:8px;background:rgba(255,255,255,0.1);transition:all .3s ease;font-weight:600}nav.nav a.nav-link:hover{background-color:#004080;transform:scale(1.05)}.banner-container{position:relative;text-align:center;margin:2rem 0;overflow:hidden;border-radius:15px;box-shadow:0 8px 24px rgba(0,0,0,0.15)}.banner-container img.banner{width:100%;display:block;filter:brightness(0.95)}.banner-caption{position:absolute;bottom:1rem;left:50%;transform:translateX(-50%);background:rgba(0,0,0,0.6);padding:.8rem 1.2rem;border-radius:6px;font-size:1.25rem;color:#fff;backdrop-filter:blur(5px)}main.main-content{padding:2rem 0;animation:fadeIn 1s ease-in-out}main.main-content .container{background-color:#fff;padding:2rem;border-radius:12px;box-shadow:0 4px 12px rgba(0,0,0,0.1);transition:transform .3s}main.main-content .container:hover{transform:translateY(-5px)}footer.footer{background-color:#2f2f2f;color:#ccc;text-align:center;padding:2rem 0;font-size:.95rem;margin-top:2rem}footer.footer a{color:#79d1c3;text-decoration:none;font-weight:bold}footer.footer a:hover{text-decoration:underline}.footer i{margin:0 .4rem;transition:transform .3s}.footer i:hover{transform:scale(1.2)}@keyframes fadeIn{from{opacity:0;transform:translateY(10px)}to{opacity:1;transform:translateY(0)}}@media(max-width:768px){nav.nav{flex-direction:column;gap:.5rem}}</style> </head> <body> <header class="header"> <div class="container"> <nav class="nav" role="navigation"> <a href="/snab2025/" class="nav-link">Home</a> <a href="/snab2025/schedule/" class="nav-link">Schedule</a> <a href="/snab2025/speakers/" class="nav-link">Speakers</a> <a href="/snab2025/venue/" class="nav-link">Venue</a> <a href="/snab2025/contact/" class="nav-link">Contact</a> </nav> </div> </header> <main class="main-content"> <div class="container"> <style>@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap');@media print{body{background:white!important;color:black!important;font-size:11pt!important;line-height:1.4!important;margin:0;padding:0}nav.schedule-nav,#global-toggle,button,footer{display:none!important}h1,h2,h3{color:black!important}.session-card{box-shadow:none!important;border:1px solid #ccc!important;page-break-inside:avoid}table.schedule-table{page-break-inside:avoid}.schedule-day{page-break-after:always}}body{font-family:'Inter',sans-serif;background-color:#f9fafc;color:#333;line-height:1.6}.schedule-nav{display:flex;justify-content:center;background:linear-gradient(90deg,#4e54c8,#8f94fb);padding:.75rem 0;border-radius:8px;margin-bottom:2rem;box-shadow:0 4px 12px rgba(0,0,0,0.1);position:sticky;top:0;z-index:999}.schedule-nav a{color:#fff;text-decoration:none;margin:0 1rem;font-weight:600;font-size:1.05rem;padding:.5rem 1rem;border-radius:6px;transition:background-color .3s,transform .2s}.schedule-nav a:hover{background-color:rgba(255,255,255,0.2);transform:scale(1.05)}.schedule-day h2{border-bottom:3px solid #8f94fb;padding-bottom:.5rem;margin-bottom:1.5rem}.session-card{background:white;border-radius:12px;box-shadow:0 4px 8px rgba(0,0,0,0.06);padding:1.25rem;margin-bottom:1.75rem;transition:box-shadow .3s ease,transform .2s ease}.session-card:hover{box-shadow:0 8px 16px rgba(0,0,0,0.1);transform:translateY(-4px)}.session-card h3{margin-top:0;color:#4e54c8;font-weight:600}.session-card small{color:#777}.schedule-table{width:100%;border-collapse:collapse;margin-top:1rem}.schedule-table th,.schedule-table td{padding:.75rem;border-bottom:1px solid #eee}.schedule-table th{background-color:#f0f2f5;text-align:left;font-weight:600}.coffee-break,.lunch-break,.banquet{text-align:center;font-style:italic;font-weight:500;margin:1.5rem 0;color:#555}.schedule-day{animation:fadeIn 1s ease-in-out}@keyframes fadeIn{from{opacity:0;transform:translateY(10px)}to{opacity:1;transform:translateY(0)}}.hidden-abstract{display:none!important}</style> <h1>Workshop Schedule</h1> <script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},svg:{fontCache:"global"}};</script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script> <nav class="schedule-nav"> <a href="#monday">Monday (June 2)</a> <a href="#tuesday">Tuesday (June 3)</a> <a href="#wednesday">Wednesday (June 4)</a> </nav> <div style="text-align: center; margin-bottom: 1.5rem;"> <button id="global-toggle" style="padding: 0.5rem 1.25rem; font-size: 1rem; background-color: #4e54c8; color: white; border: none; border-radius: 5px; cursor: pointer;"> Show All Abstracts </button> </div> <h2 id="monday">Monday</h2> <div class="session-card"> <h3>🎤 Opening Remarks</h3> <p><strong>08:50 - 09:00</strong></p> </div> <div class="session-card"> <h3>Session 1 – Network Model Assessment and Structure Inference<br><small>Chair · Yang Feng</small> </h3> <p><strong>09:00 - 10:40</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Qiwei Yao</strong><br> <small>London School of Economics and Political Science</small> </td> <td> <em>Goodness-of-Fit and the Best Approximation: an Adversarial Approach</em><br> <div class="abstract-content"> Abstract: Diagnostic checking for goodness-of-fit is one of the important and routine steps in building a statistical model. The most frequently used approach for checking the goodness-of-fit is the residual analysis in the context of regression analysis. However for many statistical models there exist no natural residuals, which includes the models for the underlying distributions behind data, the models for some complex dynamic structures such as the dynamic network models with dependent edges. Furthermore, there are scenarios in which there exist several competing models but none of them are the clear favourite. One then faces a task to choose the best approximation among the wrong models. We propose an adversarial approach in this paper. For checking the goodness-of-fit of a fitted model, we generate a synthetic sample from the fitted model and construct a classifier to classify the original sample and the synthetic sample into two different class. If the fitted model is adequate, the classifier will have difficulties in distinguish the two samples. For identifying the best model among several candidate models, the classifier will create a distance between the original sample and the synthetic sample generated from each of the candidate model, and the model with the shortest distance is chosen as the best approximation for the truth. </div> </td> </tr> <tr> <td> <strong>Ji Zhu</strong><br> <small>University of Michigan</small> </td> <td> <em>Hyperbolic Network Latent Space Model with Learnable Curvature</em><br> <div class="abstract-content"> Abstract: Network data is ubiquitous in various scientific disciplines, including sociology, economics, and neuroscience. Latent space models are often employed in network data analysis, but the geometric effect of latent space curvature remains a significant, unresolved issue. In this work, we propose a hyperbolic network latent space model with a learnable curvature parameter. We theoretically justify that learning the optimal curvature is essential to minimizing the embedding error across all hyperbolic embedding methods beyond network latent space models. A maximum-likelihood estimation strategy, employing manifold gradient optimization, is developed, and we establish the consistency and convergence rates for the maximum-likelihood estimators, both of which are technically challenging due to the non-linearity and non-convexity of the hyperbolic distance metric. We further demonstrate the geometric effect of latent space curvature and the superior performance of the proposed model through extensive simulation studies and an application using a Facebook friendship network. </div> </td> </tr> <tr> <td> <strong>Masaaki Imaizumi</strong><br> <small>University of Tokyo</small> </td> <td> <em>Statistical Analysis On In-Context Learning</em><br> <div class="abstract-content"> Abstract: Deep learning and artificial intelligence technologies have made great progress, and the usage of foundation models has attracted strong attention by its general ability. Motivated by this fact, mathematical understanding is required to efficiently control and develop these technologies. In this talk, I will present a statistics-based analysis of a scheme called in-context learning, which is an useful framework of meta-learning to describe foundation models. I argue that in-context learning can efficiently learn the latent structure of the data, using the property of transformers used in the learning scheme can efficiently handle the distribution of observations. </div> </td> </tr> <tr> <td> <strong>Wen Zhou</strong><br> <small>New York University</small> </td> <td> <em>Multivariate Inference of Network Moments by Subsampling</em><br> <div class="abstract-content"> Abstract: In this paper, we study the characterization of a network population by analyzing a single observed network, focusing on the counts of multiple network motifs or their corresponding multivariate network moments. We introduce an algorithm based on node subsampling to approximate the nontrivial joint distribution of the network moments, and prove its asymptotic accuracy. By examining the joint distribution of these moments, our approach captures complex dependencies among network motifs, making a significant advancement over earlier methods that rely on individual motifs marginally. This enables more accurate and robust network inference. Through real-world applications, such as comparing coexpression networks of distinct gene sets and analyzing collaboration patterns within the statistical community, we demonstrate that the multivariate inference of network moments provides deeper insights than marginal approaches, thereby enhancing our understanding of network mechanisms. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>☕ Coffee Break</h3> <p><strong>10:40 - 11:10</strong></p> </div> <div class="session-card"> <h3>Session 2 – Causal &amp; Temporal Inference in Networks<br><small>Chair · Ji Zhu</small> </h3> <p><strong>11:10 - 12:25</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Jinchi Lv</strong><br> <small>University of Southern California</small> </td> <td> <em>HNCI: High-Dimensional Network Causal Inference</em><br> <div class="abstract-content"> Abstract: The problem of evaluating the effectiveness of a treatment or policy commonly appears in causal inference applications under network interference. In this paper, we suggest the new method of high-dimensional network causal inference (HNCI) that provides both valid confidence interval on the average direct treatment effect on the treated (ADET) and valid confidence set for the neighborhood size for interference effect. We exploit the model setting in Belloni et al. (2022) and allow certain type of heterogeneity in node interference neighborhood sizes. We propose a linear regression formulation of potential outcomes, where the regression coefficients correspond to the underlying true interference function values of nodes and exhibit a latent homogeneous structure. Such a formulation allows us to leverage existing literature from linear regression and homogeneity pursuit to conduct valid statistical inferences with theoretical guarantees. The resulting confidence intervals for the ADET are formally justified through asymptotic normalities with estimable variances. We further provide the confidence set for the neighborhood size with theoretical guarantees exploiting the repro samples approach. The practical utilities of the newly suggested methods are demonstrated through simulation and real data examples. This is a joint work with Rundong Ding, Wenqin Du and Yingying Fan. </div> </td> </tr> <tr> <td> <strong>Jiaming Xu</strong><br> <small>Duke University</small> </td> <td> <em>A Proof of The Changepoint Detection Threshold Conjecture in Preferential Attachment Models</em><br> <div class="abstract-content"> Abstract: We investigate the problem of detecting and estimating a changepoint in the attachment function of a network evolving according to a preferential attachment model on $n$ vertices, using only a single final snapshot of the network. Bet et al. show that a simple test based on thresholding the number of vertices with minimum degrees can detect the changepoint when the change occurs at time $n-\Omega(\sqrt{n})$. They further make the striking conjecture that detection becomes impossible for any test if the change occurs at time $n-o(\sqrt{n}).$ Kaddouri et al. make a step forward by proving the detection is impossible if the change occurs at time $n-o(n^{1/3}).$ In this paper, we resolve the conjecture affirmatively, proving that detection is indeed impossible if the change occurs at time $n-o(\sqrt{n}).$ Furthermore, we establish that estimating the changepoint with an error smaller than $o(\sqrt{n})$ is also impossible, thereby confirming that the estimator proposed in Bhamidi et al. is order-optimal. </div> </td> </tr> <tr> <td> <strong>Yuting Wei</strong><br> <small>University of Pennsylvania</small> </td> <td> <em>To intrinsic dimension and beyond: Efficient sampling in diffusion models</em><br> <div class="abstract-content"> Abstract: The denoising diffusion probabilistic model (DDPM) has become a cornerstone of generative AI. While sharp convergence guarantees have been established for DDPM, the iteration complexity typically scales with the ambient data dimension of target distributions, leading to overly conservative theory that fails to explain its practical efficiency. This has sparked recent efforts to understand how DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. This talk explores two key scenarios: (1) For a broad class of data distributions with intrinsic dimension k, we prove that the iteration complexity of the DDPM scales nearly linearly with k, which is optimal under the KL divergence metric; (2) For mixtures of Gaussian distributions with k components, we show that DDPM learns the distribution with iteration complexity that grows only logarithmically in k. These results provide theoretical justification for the practical efficiency of diffusion models. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>🍽️ Lunch</h3> <p><strong>12:30 - 14:00</strong></p> </div> <div class="session-card"> <h3>Session 3 – Generative, Diffusion &amp; Attention Models<br><small>Chair · Qingfeng Liu</small> </h3> <p><strong>14:05 - 15:45</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Xiaotong Shen</strong><br> <small>University of Minnesota</small> </td> <td> <em>Generative Score Inference for Multimodal Data</em><br> <div class="abstract-content"> Abstract: Accurate uncertainty quantification is essential for reliable decision-making in various supervised learning scenarios, particularly when dealing with complex multimodal data such as images and text. Current approaches often face notable limitations, including rigid assumptions and limited generalizability, constraining their effectiveness across diverse supervised learning tasks. To overcome these limitations, we introduce Generative Score Inference (GSI), a flexible inference framework capable of constructing statistically valid and informative prediction and confidence sets across a wide range of multimodal learning problems. GSI utilizes synthetic samples generated by deep generative models to approximate conditional score distributions, facilitating precise uncertainty quantification without imposing restrictive assumptions about the data or tasks. We empirically validate GSI's capabilities through two representative scenarios: hallucination detection in large language models and uncertainty estimation in image captioning. Our method achieves state-of-the-art performance in hallucination detection and robust predictive uncertainty in image captioning, and its performance is positively influenced by the quality of the underlying generative model. These findings underscore the potential of GSI as a versatile inference framework, significantly enhancing uncertainty quantification and trustworthiness in multimodal learning. Joint with Xinyu Tian of University of Minnesota. </div> </td> </tr> <tr> <td> <strong>Jiashun Jin</strong><br> <small>Carnegie Mellon University</small> </td> <td> <em>TBD</em><br> <div class="abstract-content"> <em>Abstract coming soon.</em> </div> </td> </tr> <tr> <td> <strong>Lexing Xie</strong><br> <small>Australian National University</small> </td> <td> <em>Online Attention: Processes, Graphs, and Optimization </em><br> <div class="abstract-content"> Abstract: What makes a video popular? What drives collective attention online? What are the similarities and differences between clicks and transactions in a market? This talk aims to address these three questions. First, I will discuss a physics-inspired stochastic time series model that explains and forecasts the seemingly unpredictable patterns of viewership over time. This model provides novel metrics for predicting expected popularity gains per share and assessing sensitivity to promotions. Next, I will describe new measurement studies and machine learning models that analyze how networks of online items influence each other's attention. Finally, I will introduce a macroscopic view of attention, offering mathematical descriptions of market equilibriums and distributed optimization. Our ongoing work seek computational descriptions of attention markets that can inform potential mechanisms for a healthier online ecosystem. Additionally, my group works on visualising intellectual influence and decision-making ly moral dilemmas, which opens up new questions on individual and collective attention. </div> </td> </tr> <tr> <td> <strong>Takeru Matsuda</strong><br> <small>University of Tokyo &amp; RIKEN Center for Brain Science</small> </td> <td> <em>Matrix estimation via singular value shrinkage</em><br> <div class="abstract-content"> Abstract: In this talk, I will introduce recent studies on generalization of Stein's shrinkage estimation theory to matrices. Singular value shrinkage estimators and priors are shown to be minimax and work well when the unknown matrix is close to low-rank (e.g. reduced-rank regression). Further generalization to tensors will be also discussed. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>☕ Coffee Break</h3> <p><strong>15:45 - 16:15</strong></p> </div> <div class="session-card"> <h3>Session 4 – Reception &amp; Poster Session<br><small>Chair · Yang Feng</small> </h3> <p><strong>16:15 - 17:45</strong></p> <table class="schedule-table"> <thead> <tr> <th>Name</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Takuya Koriyama</strong><br> <small>University of Chicago</small> </td> <td> <em>Precise Asymptotics of Bagging Regularized M-estimators</em><br> <div class="abstract-content"> Abstract: We characterize the squared prediction risk of ensemble estimators obtained through subagging (subsample bootstrap aggregating) regularized M-estimators and construct a consistent estimator for the risk. Specifically, we consider a heterogeneous collection of $M \ge 1$ regularized M-estimators, each trained with (possibly different) subsample sizes, convex differentiable losses, and convex regularizers. We operate under the proportional asymptotics regime, where the sample size $n$, feature size $p$, and subsample sizes $k_m$ for $m \in [M]$ all diverge with fixed limiting ratios $n/p$ and $k_m/n$. Key to our analysis is a new result on the joint asymptotic behavior of correlations between the estimator and residual errors on overlapping subsamples, governed through a (provably) contractible nonlinear system of equations. Of independent interest, we also establish convergence of trace functionals related to degrees of freedom in the non-ensemble setting (with $M = 1$) along the way, extending previously known cases for square loss and ridge, lasso regularizers. When specialized to homogeneous ensembles trained with a common loss, regularizer, and subsample size, the risk characterization sheds some light on the implicit regularization effect due to the ensemble and subsample sizes $(M,k)$. For any ensemble size $M$, optimally tuning subsample size yields sample-wise monotonic risk. For the full-ensemble estimator (when $M \to \infty$), the optimal subsample size $k^\star$ tends to be in the overparameterized regime $(k^\star \le \min\{n,p\})$, when explicit regularization is vanishing. Finally, joint optimization of subsample size, ensemble size, and regularization can significantly outperform regularizer optimization alone on the full data (without any subagging). </div> </td> </tr> <tr> <td> <strong>Issey Sukeda</strong><br> <small>University of Tokyo</small> </td> <td> <em>Torus graph modeling for EEG analysis</em><br> <div class="abstract-content"> Abstract: Identifying phase coupling recorded from multiple electrodes during electrophysiological diagnostics, such as electroencephalogram (EEG) and electrocorticography (ECoG), helps neuroscientists and clinicians understand the underlying brain structures or mechanisms. From a statistical perspective, these signals are multi-dimensional circular measurements that are correlated with one another and can be effectively modeled using a torus graph model designed for circular random variables. Using the torus graph model avoids the issue of detecting pseudo correlations. However, the naive estimation of this model tends to lead to a dense network structure, which is difficult to interpret. Therefore, to enhance the interpretability of the brain network structure, we propose to induce a sparse solution by implementing a regularized score matching estimation for the torus graph model based on the information criteria. In numerical simulations, our method successfully recovered the true dependence structure of the brain, from a synthetic dataset sampled from a pre-given torus graph model distribution. Furthermore, we present analyses of two real datasets, one involving human EEG and the other marmoset ECoG, demonstrating that our method can be widely applied to phase-coupling analysis across different types of neural data. Using our proposed method, the modularity of the estimated network structure revealed more resolved brain structures and demonstrated differences in trends among individuals. </div> </td> </tr> <tr> <td> <strong>Ergan Shang</strong><br> <small>Carnegie Mellon University</small> </td> <td> <em>Inference for Balance Theory in Time-Varying Signed Network</em><br> <div class="abstract-content"> Abstract: Dynamic signed networks are frequently used nowadays to describe the trend of two types of relationship, for example, alliances and disputes respectively, as time varies, using both positive edges and negative ones. To understand the connectivity property of signed network in different timestamps, the social balance theory considers the fully connected 3-node coliques and evaluate the portion of balanced triangles. To alleviate the hard assumption of relatively large sparsity level for making inference, we set up a statistical dynamic signed network model and propose a kernel smoothing estimator to make inference on the balanced portion parameter at the time even not observed in the data. In the process, we also give a bandwidth selection method to use in practice. Moreover, the theoretical guarantee of the error rate of our method is boosted by leveraging multiple graphs for inference under the alleviated requirement on the sparsity parameter. Finally, we apply our method on simulation studies and a real dataset of international alliances and disputes in the area of political science to exploit the inference power of our methodology. </div> </td> </tr> <tr> <td> <strong>Tao Shen</strong><br> <small>National University of Singapore</small> </td> <td> <em>Optimal Network-Guided Covariate Selection for High-Dimensional Data Integration</em><br> <div class="abstract-content"> Abstract: When integrating datasets from different studies, it is common that they have components of different formats. How to combine them organically for improved estimation is important and challenging. This work investigates this problem in a two-study scenario, where covariates are observed for all subjects, but network data is available in only one study, and response variables are available only in the other. To leverage the partially observed network information, we propose the Network-Guided Covariate Selection (NGCS) algorithm. It integrates the spectral information from network adjacency matrices with the Higher Criticism Thresholding approach for informative covariates identification. Theoretically, we prove that NGCS achieves the optimal rate in covariate selection, which is the same rate in the supervised learning setting. Furthermore, this optimality is robust to network models and tuning parameters. This framework extends naturally to clustering and regression tasks, with two proposed algorithms: NG-clu and NG-reg. Empirical studies on synthetic and real-world datasets demonstrate the robustness and superior performance of our algorithms, underscoring their effectiveness in handling heterogeneous data formats. </div> </td> </tr> <tr> <td> <strong>Fan Wang</strong><br> <small>University of Warwick</small> </td> <td> <em>Change Point Analysis in Dynamic Multilayer Networks</em><br> <div class="abstract-content"> Abstract: In this talk, I will introduce the multilayer random dot product graph (MRDPG) model, a generalization of the random dot product graph model to multilayer networks. To estimate edge probabilities, I will present a tensor-based methodology and demonstrate its superiority over existing approaches. Moving to dynamic MRDPGs, I will formulate and analyze an online change point detection framework, where, at each time point, a realization from an MRDPG is observed. I will propose a novel nonparametric change point detection algorithm based on density kernel estimators and tensor-based methods. This approach is broadly applicable to various network settings, including stochastic block models as special cases. Theoretically, I will show that our methods effectively minimize the detection delay while controlling false alarms. Extensive numerical experiments, including an application to U.S. air transportation networks, supported our theoretical findings. </div> </td> </tr> <tr> <td> <strong>Guillaume Braun</strong><br> <small>RIKEN Center for Advanced Intelligence Project</small> </td> <td> <em>VEC-SBM: Optimal Community Detection with Vectorial Edge Covariates</em><br> <div class="abstract-content"> Abstract: Social networks are often associated with rich side information, such as texts and images. While numerous methods have been developed to identify communities from pairwise interactions, they usually ignore such side information. In this work, we study an extension of the Stochastic Block Model (SBM), a widely used statistical framework for community detection, that integrates vectorial edges covariates: the Vectorial Edges Covariates Stochastic Block Model (VEC-SBM). We propose a novel algorithm based on iterative refinement techniques and show that it optimally recovers the latent communities under the VEC-SBM. Furthermore, we rigorously assess the added value of leveraging edge's side information in the community detection process. We complement our theoretical results with numerical experiments on synthetic and semi-synthetic data. </div> </td> </tr> <tr> <td> <strong>Bingcheng Sui</strong><br> <small>University of Science and Technology of China </small> </td> <td> <em>Counting Cycles with AI</em><br> <div class="abstract-content"> Abstract: Despite recent progress, AI still struggles on advanced mathematics. We consider a difficult open problem: how to derive an equivalent expression for the cycle count statistics that leads to a computational approach with minimal cost. The problem does not have known general solutions, and requires delicate combinatorics and tedious calculations. Such a task is hard to accomplish by human but is an ideal example where AI can be very helpful. We solve the problem by combining a novel approach we propose and the powerful coding skills of AI. Our results use delicate graph theory and contain new formula for general cases that have not been discovered before. We find that, while AI is unable to solve the problem all by itself, it is able to solve it if we provide it with a clear strategy, a step-by-step guidance and carefully written prompts. For simplicity, we focus our study on DeepSeek-R1 but we also investigate other AI approaches. </div> </td> </tr> </tbody> </table> </div> <h2 id="tuesday">Tuesday</h2> <div class="session-card"> <h3>Session 5 – High-Dimensional Statistics and Tensor Methods<br><small>Chair · Jiashun Jin</small> </h3> <p><strong>09:00 - 10:40</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Cun-Hui Zhang</strong><br> <small>Rutgers University</small> </td> <td> <em>Simultaneous Decorrelation of Matrix Time Series</em><br> <div class="abstract-content"> Abstract: We propose a contemporaneous bilinear transformation for a p-by-q matrix time series to alleviate the difficulties in modeling and forecasting matrix time series when p and/or q are large. The resulting transformed matrix assumes a block structure consisting of several small matrices, and those small matrix series are uncorrelated across all times. Hence an overall parsimonious model is achieved by modelling each of those small matrix series separately without the loss of information on the linear dynamics. Such a parsimonious model often has better forecasting performance, even when the underlying true dynamics deviates from the assumed uncorrelated block structure after transformation. The uniform convergence rates of the estimated transformation are derived. The proposed method is illustrated numerically via both simulated and real data examples. This is joint work with Yuefeng Han, Rong Chen and Qiwei Yao. </div> </td> </tr> <tr> <td> <strong>Lexin Li</strong><br> <small>University of California Berkeley</small> </td> <td> <em>Tensor Data Analysis and Some Applications in Neuroscience</em><br> <div class="abstract-content"> Abstract: Multidimensional arrays, or tensors, are becoming increasingly prevalent in a wide range of scientific applications. In this talk, I will present two case studies from neuroscience, where tensor decomposition proves particularly useful. The first study is a cross-area neuronal spike trains analysis, which we formulate as the problem of regressing a multivariate point process on another multivariate point process. We model the predictor effects through the conditional intensities using a set of basis transferring functions in a convolutional fashion. We then organize the corresponding transferring coefficients in the form of a three-way tensor, and impose the low-rank, sparsity, and subgroup structures on this coefficient tensor. The second study is a multimodal neuroimaging analysis for Alzheimer's disease, which we formulate as the problem of modeling the correlations of two sets of variables conditioning on the third set of variables. We propose a generalized liquid association analysis method to study such three-way associations. We establish a population dimension reduction model, and transform the problem to sparse decomposition of a three-way tensor. </div> </td> </tr> <tr> <td> <strong>Jeff Yao</strong><br> <small>Chinese University of Hong Kong Shenzhen</small> </td> <td> <em>Alignment and matching tests for high-dimensional tensor signals via tensor contraction</em><br> <div class="abstract-content"> Abstract: We consider two hypothesis testing problems for low-rank and high-dimensional tensor signals, namely the tensor signal alignment and tensor signal matching problems. These problems are challenging due to the high dimension of tensors and lack of meaningful test statistics. By exploiting a recent tensor contraction method, we propose and validate relevant test statistics using eigenvalues of a data matrix resulting from the tensor contraction. The matrix has a long range dependence among its entries, which makes the analysis of the matrix challenging, involved and distinct from standard random matrix theory. Our approach provides a novel framework for addressing hypothesis testing problems in the context of high-dimensional tensor signals. This is a joint work with Ruihan Liu (The University of Hong Kong) and Zhenggang Wang (Southeast University). Full paper available at https://arxiv.org/abs/2411.01732 </div> </td> </tr> <tr> <td> <strong>Yuqi Gu</strong><br> <small>Columbia University</small> </td> <td> <em>Minimax-Optimal Dimension-Reduced Clustering for High-Dimensional Nonspherical Mixtures</em><br> <div class="abstract-content"> Abstract: In mixture models, nonspherical (anisotropic) noise within each cluster is widely present in real-world data. We study both the minimax rate and optimal statistical procedure for clustering under high-dimensional nonspherical mixture models. In high-dimensional settings, we first establish the information-theoretic limits for clustering under Gaussian mixtures. The minimax lower bound unveils an intriguing informational dimension-reduction phenomenon: there exists a substantial gap between the minimax rate and the oracle clustering risk, with the former determined solely by the projected centers and projected covariance matrices in a low-dimensional space. Motivated by the lower bound, we propose a novel computationally efficient clustering method: Covariance Projected Spectral Clustering (COPO). Its key step is to project the high-dimensional data onto the low-dimensional space spanned by the cluster centers and then use the projected covariance matrices in this space to enhance clustering. We establish tight algorithmic upper bounds for COPO, both for Gaussian noise with flexible covariance and general noise with local dependence. Our theory indicates the minimax-optimality of COPO in the Gaussian case and highlights its adaptivity to a broad spectrum of dependent noise. Extensive simulation studies under various noise structures and real data analysis demonstrate our method's superior performance. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>☕ Coffee Break</h3> <p><strong>10:40 - 11:10</strong></p> </div> <div class="session-card"> <h3>Session 6 – Privacy-Preserving &amp; Dynamic Network Learning<br><small>Chair · Xiaoyue Niu</small> </h3> <p><strong>11:10 - 12:25</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Tony Cai</strong><br> <small>University of Pennsylvania</small> </td> <td> <em>TBD</em><br> <div class="abstract-content"> <em>Abstract coming soon.</em> </div> </td> </tr> <tr> <td> <strong>Yi Yu</strong><br> <small>University of Warwick</small> </td> <td> <em>Optimal federated learning under differential privacy constraints </em><br> <div class="abstract-content"> Abstract: With the growing computational power and increasing awareness of privacy, federated learning has emerged as a pivotal framework for private, distributed data analysis. Depending on applications, diverse privacy constraints come into play, each imposing a unique cost on statistical accuracy. In this talk, I will start with an overview of the foundational concept of differential privacy (DP). I will then introduce three notions of DP tailored to the federated learning context, highlighting their relevance and implications in distributed settings. The core focus of this talk will be on a functional data estimation problem under a hierarchical and heterogeneous DP framework. I will discuss how privacy constraints impact estimation accuracy and quantify these tradeoffs through the lens of minimax theory. Key aspects of the proofs will also be outlined, as well as some numerical performances. </div> </td> </tr> <tr> <td> <strong>Jinyuan Chang</strong><br> <small>Southwestern University of Finance and Economics</small> </td> <td> <em>Autoregressive Networks with Dependent Edges</em><br> <div class="abstract-content"> Abstract: We propose an autoregressive framework for modelling dynamic networks with dependent edges. It encompasses models that accommodate, for example, transitivity, density-dependence and other stylized features often observed in real network data. By assuming the edges of networks at each time are independent conditionally on their lagged values, the models, which exhibit a close connection with temporal ERGMs, facilitate both simulation and the maximum likelihood estimation in a straightforward manner. Due to the possibly large number of parameters in the models, the natural MLEs may suffer from slow convergence rates. An improved estimator for each component parameter is proposed based on an iteration employing projection, which mitigates the impact of the other parameters (Chang et al., 2021, 2023). Leveraging a martingale difference structure, the asymptotic distribution of the improved estimator is derived without the assumption of stationarity. The limiting distribution is not normal in general, although it reduces to normal when the underlying process satisfies some mixing conditions. Illustration with a transitivity model was carried out in both simulation and a real network data set. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>📸 Group Photo</h3> <p><strong>12:25 - 12:30</strong></p> </div> <div class="session-card"> <h3>🍽️ Lunch</h3> <p><strong>12:30 - 14:00</strong></p> </div> <div class="session-card"> <h3>Session 7 – Data Integration &amp; Applications<br><small>Chair · Emma Jingfei Zhang</small> </h3> <p><strong>14:05 - 15:45</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Annie Qu</strong><br> <small>University of California Irvine</small> </td> <td> <em>High-order Joint Embedding for Multi-Level Link Prediction</em><br> <div class="abstract-content"> Abstract: Link prediction infers potential links from observed networks, and is one of the essential problems in network analyses. In contrast to traditional graph representation modeling which only predicts two-way pairwise relations, we propose a novel tensor-based joint network embedding approach on simultaneously encoding pairwise links and hyperlinks onto a latent space, which captures the dependency between pairwise and multi-way links in inferring potential unobserved hyperlinks. The major advantage of the proposed embedding procedure is that it incorporates both the pairwise relationships and subgroup-wise structure among nodes to capture richer network information. In addition, the proposed method introduces a hierarchical dependency among links to infer potential hyperlinks, and leads to better link prediction. In theory we establish the estimation consistency for the proposed embedding approach, and provide a faster convergence rate compared to link prediction utilizing pairwise links or hyperlinks only. Numerical studies on both simulation settings and Facebook ego-networks indicate that the proposed method improves both hyperlink and pairwise link prediction accuracy compared to existing link prediction algorithms. This is a joint work with Prof. Yubai Yuan at Penn State. </div> </td> </tr> <tr> <td> <strong>Peter Song</strong><br> <small>University of Michigan</small> </td> <td> <em>Network Structural Equation Models for Causal Mediation and Spillover Effects </em><br> <div class="abstract-content"> Abstract: Social network interference induces spillover effects from neighbors' exposures, and the complexity of statistical analysis increases when mediators are involved with network interference. We develop a theoretical framework employing a structural graphical modeling approach to investigate both mediation and interference effects within network data. Our framework enables us to capture the multifaceted mechanistic pathways through which neighboring units' exposures and mediators exert direct and indirect influences on an individual's outcome. We extend the exposure mapping paradigm in the context of a random-effects network structural equation models (REN-SEM), establishing its capacity to delineate spillover effects of interest. Identifiability conditions for both causal mediation and interference estimands are postulated rigorously. Our proposed methodology contributions include maximum likelihood estimation for REN-SEM and inference procedures with theoretical guarantees. Such guarantees encompass consistent asymptotic variance estimators, derived under a non-i.i.d. asymptotic theory. The robustness and practical utility of our methodology are demonstrated through simulation experiments, underscoring its effectiveness in capturing the intricate dynamics of network-mediated exposure effects. </div> </td> </tr> <tr> <td> <strong>Tracy Ke</strong><br> <small>Harvard University</small> </td> <td> <em>Poisson-Process Topic Model for Integrating Knowledge from Pre-trained Language Models</em><br> <div class="abstract-content"> Abstract: Topic modeling is traditionally applied to word counts without accounting for the context in which words appear. Recent advancements in large language models (LLMs) offer contextualized word embeddings, which capture deeper meaning and relationships between words. We aim to leverage such embeddings to improve topic modeling. We use a pre-trained LLM to convert each document into a sequence of word embeddings. This sequence is then modeled as a Poisson point process, with its intensity measure expressed as a convex combination of K base measures, each corresponding to a topic. To estimate these topics, we propose a flexible algorithm that integrates traditional topic modeling methods, enhanced by net-rounding applied before and kernel smoothing applied after. One advantage of this framework is that it treats the LLM as a black box, requiring no fine-tuning of its parameters. Another advantage is its ability to seamlessly integrate any traditional topic modeling approach as a plug-in module, without the need for modifications. Assuming each topic is a β-Hölder smooth intensity measure on the embedded space, we establish the rate of convergence of our method. We apply our method to several datasets, providing evidence that it offers an advantage over traditional topic modeling approaches. </div> </td> </tr> <tr> <td> <strong>Hui Shen</strong><br> <small>McGill University</small> </td> <td> <em>Consistent Identification of Top-K Nodes in Noisy Networks</em><br> <div class="abstract-content"> Abstract: Identifying the most important nodes in a network, often defined via centrality measures, is a fundamental challenge in applied network analysis. However, real-world networks are frequently constructed from noisy or incomplete data, leading to distortions in centrality rankings and misidentification of key nodes. In this paper, we systematically examine how network noise affects the accurate recovery of the true top-$k$ node set, as measured by degree centrality. Specifically, we analyze the performance of the empirical top-$k$ set obtained from a noisy observation of the network, where edges are randomly added or removed under a probabilistic noise model. We derive conditions for consistent recovery and characterize regimes where reliable identification is provably impossible. To assess the stability of the empirical ranking, we establish sharp lower and upper bounds on the expected set difference between the empirical and true top-$k$ sets, both in general and under specific network models. Simulation studies corroborate our theoretical findings and demonstrate the practical value of these bounds across a variety of network settings. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>☕ Coffee Break</h3> <p><strong>15:45 - 16:15</strong></p> </div> <div class="session-card"> <h3>Session 8 – Spectral / Hypergraph &amp; Signal-Detection Methods<br><small>Chair · Rachel Wang</small> </h3> <p><strong>16:15 - 17:30</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Emma Jingfei Zhang</strong><br> <small>Emory University</small> </td> <td> <em>Modeling Non-Uniform Hypergraphs Using Determinantal Point Processes</em><br> <div class="abstract-content"> Abstract: Most statistical models for networks focus on pairwise interactions between nodes. However, many real-world networks involve higher-order interactions among multiple nodes, such as co-authors collaborating on a paper. Hypergraphs provide a natural representation for these networks, with each hyperedge representing a set of nodes. The majority of existing hypergraph models assume uniform hyperedges (i.e., edges of the same size) or rely on diversity among nodes. In this work, we propose a new hypergraph model based on non-symmetric determinantal point processes. The proposed model naturally accommodates non-uniform hyperedges, has tractable probability mass functions, and accounts for both node similarity and diversity in hyperedges. For model estimation, we maximize the likelihood function under constraints using a computationally efficient projected adaptive gradient descent algorithm. We establish the consistency and asymptotic normality of the estimator. Simulation studies confirm the efficacy of the proposed model, and its utility is further demonstrated through edge predictions on several real-world datasets. </div> </td> </tr> <tr> <td> <strong>Zhigang Bao</strong><br> <small>Hong Kong University of Science and Technology</small> </td> <td> <em>Signal Detection from Spiked Noise via Asymmetrization</em><br> <div class="abstract-content"> Abstract: The signal-plus-noise model is a fundamental framework in signal detection, where a low-rank signal is corrupted by noise. In the high-dimensional setting, one often uses the leading singular values and corresponding singular vectors of the data matrix to conduct statistical inference on the signal component. Specifically, when the noise consists of i.i.d. random entries, the singular values of the signal component can be estimated from those of the data matrix, provided the signal is sufficiently strong. However, when the noise entries are heteroscedastic or correlated, this standard approach may fail. In particular, this talk considers a challenging scenario that arises with heteroscedastic noise: when the noise itself can create spiked singular values. This raises the recurring question of how to distinguish the signal from the spikes in the noise. To address this, we study the eigenvalues of an asymmetrized model when two samples are available. We demonstrate that by examining the leading eigenvalues (in magnitude) of the asymmetrized model, one can reliably detect the signal. This approach is effective even in the heavy-tailed regime, where the singular value method fails. </div> </td> </tr> <tr> <td> <strong>Jiaoyang Huang</strong><br> <small>University of Pennsylvania</small> </td> <td> <em>Fast convergence for high-order ODE solvers in diffusion probabilistic models.</em><br> <div class="abstract-content"> Abstract: Diffusion probabilistic models generate samples (e.g., of images or audio) that follow an unknown target data distribution by defining a forward process that gradually diffuses data into noise, followed by learning a reverse-time dynamics process that samples data from noise. By formulating the reverse dynamics as a deterministic probability flow ordinary differential equation (ODE), these models enable efficient sampling using high-order numerical solvers, typically requiring only 10-50 steps to create high-quality samples. Since the score function, used in the reverse-time dynamics, is typically approximated by a neural network, understanding the interplay between its regularity, approximation error, and the numerical integration error is essential to analyzing the overall sampling process. In this talk we discuss our work on the analysis of the convergence properties of the deterministic sampling methods derived from probability flow ODEs, focusing on higher order (exponential) Runge-Kutta solvers. Our results are general, holding for arbitrary forward processes as well as flexible learning rate schedules. This is a joint work with Daniel Zhengyu Huang and Zhengjiang Lin. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>🎉 Banquet</h3> <p><strong>18:30 - 21:00</strong></p> </div> <h2 id="wednesday">Wednesday</h2> <div class="session-card"> <h3>Session 9 – Complex Networks and Multilayer Networks<br><small>Chair · Kaizheng Wang</small> </h3> <p><strong>09:00 - 10:40</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Xinyu Zhang</strong><br> <small>Chinese Academy of Sciences</small> </td> <td> <em>A Transfer Learning Framework for Multilayer Networks via Model Averaging</em><br> <div class="abstract-content"> Abstract: Link prediction in multilayer networks is a key challenge in applications such as recommendation systems and protein-protein interaction prediction. While many techniques have been developed, most rely on assumptions about shared structures and require access to raw auxiliary data, limiting their practicality. To address these issues, we propose a transfer learning framework for multilayer networks using a bi-level model averaging method. Our approach introduces a cross-validation criterion based on edges to automatically weight inter-layer and intra-layer candidate models. Theoretically, we prove the optimality and weight convergence of our method under mild conditions. Computationally, our framework is efficient and privacy-preserving, as it avoids raw data sharing and supports parallel processing across multiple servers. Simulations show our method outperforms others in predictive accuracy and robustness. We further demonstrate its practical value through two real-world recommendation system applications. </div> </td> </tr> <tr> <td> <strong>Rachel Wang</strong><br> <small>University of Sydney</small> </td> <td> <em>Network autoregression for the propagation of binary responses in social networks</em><br> <div class="abstract-content"> Abstract: Studying the propagation of binary responses on nodes in a large-scale social network is critical for understanding how individual behaviors and decisions are shaped by social structures and for predicting collective outcomes. We propose a network autoregressive model for binary-valued responses, in which the probability of response at each node is influenced by its neighbors' past decisions, its own past decision, and node-specific covariates, through a logistic link function. The model accounts for network noise and community structure by assuming the underlying network is generated from a block model, with autoregressive parameters that are community-specific. We establish conditions under which the long term behavior of the high-dimensional binary vector converges to a community-specific distribution and the associated convergence rate, illustrating when individuals in the same community or across the whole network reach a consensus regardless of their initial positions. Given an observed network and response vectors, we show asymptotic consistency and normality of the maximum likelihood estimators. We demonstrate the efficiency and validity of the inference procedure through simulated and real data. In particular, we show the model can be used to study the dynamics of strike occurrences in China and highlight the impact of online social network in facilitating collective actions. </div> </td> </tr> <tr> <td> <strong>Ji Oon Lee</strong><br> <small>Korea Advanced Institute of Science &amp; Technology</small> </td> <td> <em>Detection problems in spiked Wigner matrices</em><br> <div class="abstract-content"> Abstract: The spiked Wigner matrix model is one of the most basic yet fundamental models for signal-plus-noise type data, where the signal is a vector and the noise is a symmetric random matrix. One of the main questions in the study of the spiked Wigner matrices is the detection problem, where the main goal is to detect the presence or the absence of the signal in a given data matrix. In this talk, I will explain various mathematical results on the detection problem, such as the fundamental limit and detecting algorithms, which are based on the study of random matrices and spin glass models. </div> </td> </tr> <tr> <td> <strong>Wanjie Wang</strong><br> <small>National University of Singapore</small> </td> <td> <em>Data Integration: Network-Guided Covariate Selection</em><br> <div class="abstract-content"> Abstract: In the era of data, the integration of data becomes more and more important. The data may come from different studies, different clients, or the same client but on different aspects. In this work, we are interested in the social platform, where we can observe both the user-user connection data (network) and the user profile/tags/posts (covariates). Our question arises: can we find the influential covariates, i.e. covariates that are related to the hidden information of users? Without network information, this is an unsupervised learning problem. Based on the standard procedure, we propose a network-guided covariate selection algorithm. Leveraging the network information, we significantly improve the selection power. The algorithm is efficient and robust to various network models. Finally, we discuss the downstream applications with selected covariates, including clustering and regression. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>☕ Coffee Break</h3> <p><strong>10:40 - 11:10</strong></p> </div> <div class="session-card"> <h3>Session 10 – Advances in Statistical Learning and Uncertainty Quantification<br><small>Chair · Qiyang Han</small> </h3> <p><strong>11:10 - 12:25</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Xinghua Zheng</strong><br> <small>Hong Kong University of Science and Technology</small> </td> <td> <em>Stock Co-jump Network Models based on Site Percolation</em><br> <div class="abstract-content"> Abstract: Stock prices often exhibit co-jumps, even in the absence of market jumps. To capture such phenomena, we introduce a class of network models based on site-percolation, which contrasts with usual network models that rely on edge-based connections to represent dependencies. We discuss the fundamental differences between these two modeling approaches, develop community detection methods tailored to our proposed framework, and demonstrate their economic significance through empirical applications. </div> </td> </tr> <tr> <td> <strong>Robert Lunde</strong><br> <small>Washington University at St Louis</small> </td> <td> <em>Conformal Prediction for Dyadic Regression Under Structured Missingness</em><br> <div class="abstract-content"> Abstract: Dyadic regression, which involves modeling a relational matrix given covariate information, is an important task in statistical network analysis. We consider uncertainty quantification for dyadic regression models using conformal prediction. We establish finite-sample validity of our procedures for various sampling mechanisms under a joint exchangeability assumption. Our proof uses new results related to the validity of conformal prediction beyond exchangeability, which may be of independent interest. We also show that, under certain conditions, it is possible to construct asymptotically valid prediction sets for a missing entry under a structured missingness assumption. </div> </td> </tr> <tr> <td> <strong>Kaizheng Wang</strong><br> <small>Columbia University</small> </td> <td> <em>Uncertainty Quantification for LLM-Based Survey Simulations</em><br> <div class="abstract-content"> Abstract: We investigate the reliable use of simulated survey responses from large language models (LLMs) through the lens of uncertainty quantification. Our approach converts synthetic data into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>🍽️ Lunch</h3> <p><strong>12:30 - 14:00</strong></p> </div> <div class="session-card"> <h3>Session 11 – Graphical Models, Hypergraphs, and Generative Approaches<br><small>Chair · Xinyu Zhang</small> </h3> <p><strong>14:05 - 15:20</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Nick Whiteley</strong><br> <small>University of Edinburgh</small> </td> <td> <em>Statistical exploration of the Manifold Hypothesis</em><br> <div class="abstract-content"> Abstract: The Manifold Hypothesis is a widely held tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the true data generating mechanism. This is joint work with Annie Gray and Patrick Rubin-Delanchy. </div> </td> </tr> <tr> <td> <strong>Jingming Wang</strong><br> <small>University of Virginia</small> </td> <td> <em>Network Goodness-of-Fit for the block-model family</em><br> <div class="abstract-content"> Abstract: The block model family is widely used in network modeling and includes four popular models: SBM, DCBM, MMSBM and DCMM. However, the question of which block model best fits real networks has received limited attention in the literature. In this talk, I will introduce a novel approach using cycle count statistics to address the Goodness-of-Fit for these block models. By leveraging the cycle count statistics and a network fitting scheme, we construct four GoF metrics with parameter-free limiting distributions of $N(0,1)$ under the assumed models. We apply these GoF-metrics to some frequently-used real networks for comparison. The numerical results suggest that DCMM is particularly promising for modeling undirected networks. This talk is based on joint work with Jiashun Jin, Tracy Ke, and Jiajun Tang. </div> </td> </tr> <tr> <td> <strong>Yao Xie</strong><br> <small>Georgia Institute of Technology</small> </td> <td> <em>Scalable flow-based generative models for network data</em><br> <div class="abstract-content"> Abstract: Generative models have become a transformative technology in machine learning, with significant advancements made for vector-valued data. However, generative modeling for network data, i.e., data with underlying graph topology remains an emerging area. In this work, we develop a deep generative model, the invertible graph neural network(iGNN), for network data by formulating it as a conditional generative task based on flow-based generative models. The proposed model is capable of generating synthetic samples and performing probabilistic prediction by capturing correlations among nodal observations while quantifying uncertainty. It consists of an invertible sub-network that establishes a one-to-one mapping from data to encoded features, enabling forward prediction via a linear classification sub-network and efficient generation from output labels through a parametric mixture model. The invertibility of the encoding sub-network is enforced via Wasserstein-2 regularization, which allows flexible, free-form layers in the residual blocks. A key feature of the model is its scalability to large graphs, achieved through a factorized parametric mixture model of the encoded features, along with computational efficiency provided by GNN layers. The existence of an invertible flow mapping is supported by theories from optimal transport and diffusion processes, and we prove the expressiveness of graph convolution layers in approximating the theoretical flows of graph data. The proposed model is evaluated on synthetic datasets, including large-scale graph examples, and its empirical advantages are demonstrated on real-world applications such as solar ramping event prediction and traffic flow anomaly detection. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>☕ Coffee Break</h3> <p><strong>15:20 - 15:50</strong></p> </div> <div class="session-card"> <h3>Session 12 – Random Matrix Theory and Differential Privacy<br><small>Chair · Wanjie Wang</small> </h3> <p><strong>15:50 - 17:05</strong></p> <table class="schedule-table"> <thead> <tr> <th>Speaker</th> <th>Title</th> </tr> </thead> <tbody> <tr> <td> <strong>Qiyang Han</strong><br> <small>Rutgers University</small> </td> <td> <em>Algorithmic inference via gradient descent: from linear regression to neural networks</em><br> <div class="abstract-content"> Abstract: Classical statistical methods typically rely on constructing a single estimator with desirable properties, assuming a clear separation between statistical optimality and algorithmic achievability. In contrast, modern machine learning applies simple first-order methods to complex non-convex models, often without any provable guarantees of convergence to traditional empirical risk minimizers. Can we still perform precise statistical inference in such settings? This talk introduces a new framework for algorithmic inference using the simplest gradient descent algorithm. Under suitable random data models, we show that gradient descent can be augmented, via a few auxiliary computations, to perform various valid inference tasks. We illustrate this approach in both classical linear and logistic regression models, as well as in significantly more complex multi-layer neural network models. In both cases, the augmented gradient descent algorithm produces, at each iteration, consistent estimates of the generalization error and valid confidence intervals for the unknown signal. In principle, these iteration-wise estimates can inform practical decisions such as early stopping and hyperparameter tuning during gradient descent training. This new algorithmic inference approach relies on a recent state evolution theory for a class of General First-Order Methods (GFOMs) developed by the author. </div> </td> </tr> <tr> <td> <strong>Yumou Qiu</strong><br> <small>Peking University</small> </td> <td> <em>Versatile differentially private learning for general loss functions</em><br> <div class="abstract-content"> Abstract: This paper aims to provide a versatile privacy-preserving release mechanism along with a unified approach for subsequent parameter estimation and statistical inference. We propose the ZIL privacy mechanism based on zero-inflated symmetric multivariate Laplace noise, which requires no prior specification of subsequent analysis tasks, allows for general loss functions under minimal conditions, imposes no limit on the number of analyses, and is adaptable to the increasing data volume in online scenarios. We derive the trade-off function for the proposed ZIL mechanism that characterizes its privacy protection level. Within the M-estimation framework, we propose a novel doubly random corrected loss (DRCL) for the ZIL mechanism, which provides consistent and asymptotic normal M-estimates for the parameters of the target population under differential privacy constraints. The proposed approach is easy to compute without numerical integration and differentiation for noisy data. It is applicable for a general class of loss functions, including non-smooth loss functions like check loss and hinge loss. Simulation studies, including logistic regression and quantile regression, are conducted to evaluate the performance of the proposed method. </div> </td> </tr> <tr> <td> <strong>Guangming Pan</strong><br> <small>Nanyang Technological University</small> </td> <td> <em>EIGENVECTOR OVERLAPS IN LARGE SAMPLE COVARIANCE MATRICES</em><br> <div class="abstract-content"> Abstract: Consider a data matrix $Y = [\mathbf{y}_1, \cdots, \mathbf{y}_N]$ of size $M \times N$, where the columns are independent observations from a random vector $\mathbf{y}$ with zero mean and population covariance $\Sigma$. Let $u_i$ and $v_j$ denote the left and right singular vectors of $Y$, respectively. This study investigates the eigenvector/singular vector overlaps $\langle{u_i, D_1 u_j}\rangle$, $\langle{v_i, D_2 v_j}\rangle$ and $\langle{u_i, D_3 v_j}\rangle$, where $D_k$ are general deterministic matrices with bounded operator norms. In the high-dimensional regime, where the dimension $M$ scales proportionally with the sample size $N$, we establish the convergence in probability of these eigenvector overlaps towards their deterministic counterparts with explicit convergence rates. Building on these findings, we offer a more precise characterization of the loss associated with Ledoit and Wolf's nonlinear shrinkage estimators of the population covariance $\Sigma$. </div> </td> </tr> </tbody> </table> </div> <div class="session-card"> <h3>Closing Remarks and Poster Awards</h3> <p><strong>17:05 - 17:30</strong></p> </div> <script>document.addEventListener("DOMContentLoaded",function(){const t=document.getElementById("global-toggle"),e=document.querySelectorAll(".abstract-content");let n=!1;e.forEach(t=>t.classList.add("hidden-abstract")),t.addEventListener("click",function(){e.forEach(t=>t.classList.toggle("hidden-abstract")),n=!n,t.textContent=n?"Hide All Abstracts":"Show All Abstracts"})});</script> </div> </main> <footer class="footer"> <div class="container"> <p> © SNAB - 2025 | <a href="/">Back to Main Site</a> </p> <p> <i class="fab fa-twitter"></i> <i class="fab fa-facebook"></i> <i class="fab fa-linkedin"></i> <i class="fas fa-envelope"></i> </p> </div> </footer> </body> </html>